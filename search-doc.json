[{"title":"gromacs on GPUs","type":0,"sectionRef":"#","url":"blog/gromacs-gpu","content":"","keywords":""},{"title":"Using gromacs","type":1,"pageTitle":"gromacs on GPUs","url":"blog/gromacs-gpu#using-gromacs","content":"I'll start with the end result for those of you who just want to use it but following that I'll dive into the nuts and bolts of how we created the module so you can perform additional optimizations. This is a GPU-enabled version of gromacs so we need a GPU first (can verify with nvidia-smi). srun -p build-gpu --time=4:00:00 -n 4 --mem=20G --gpus=1 --pty $0 Copy "},{"title":"gromacs-2020.4 module","type":1,"pageTitle":"gromacs on GPUs","url":"blog/gromacs-gpu#gromacs-20204-module","content":"Once we have a GPU we use modules to load gromacs-2020.4 and all its required dependencies (e.g., CUDA11). module load gromacs/2020.4-cuda11.1 Copy All packages are sub-commands of the gmx binary so you can verify the module. $ gmx -version :-) GROMACS - gmx, 2020.4 (-: GROMACS version: 2020.4 Verified release checksum is 79c2857291b034542c26e90512b92fd4b184a1c9d6fa59c55f2e24ccf14e7281 Precision: single Memory model: 64 bit MPI library: thread_mpi OpenMP support: enabled (GMX_OPENMP_MAX_THREADS = 64) GPU support: CUDA SIMD instructions: AVX_512 FFT library: fftw-3.3.3-sse2 RDTSCP usage: enabled TNG support: enabled Hwloc support: hwloc-1.11.8 Tracing support: disabled C compiler: /sw/gcc/10.1.0/bin/gcc GNU 10.1.0 C compiler flags: -mavx512f -mfma -fexcess-precision=fast -funroll-all-loops -O3 -DNDEBUG C++ compiler: /sw/gcc/10.1.0/bin/g++ GNU 10.1.0 C++ compiler flags: -mavx512f -mfma -fexcess-precision=fast -funroll-all-loops -fopenmp -O3 -DNDEBUG CUDA compiler: /sw/cuda/11.1.1-1/bin/nvcc nvcc: NVIDIA (R) Cuda compiler driver;Copyright (c) 2005-2020 NVIDIA Corporation;Built on Mon_Oct_12_20:09:46_PDT_2020;Cuda compilation tools, release 11.1, V11.1.105;Build cuda_11.1.TC455_06.29190527_0 CUDA compiler flags:-gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-Wno-deprecated-gpu-targets;-gencode;arch=compute_35,code=compute_35;-gencode;arch=compute_50,code=compute_50;-gencode;arch=compute_52,code=compute_52;-gencode;arch=compute_60,code=compute_60;-gencode;arch=compute_61,code=compute_61;-gencode;arch=compute_70,code=compute_70;-gencode;arch=compute_75,code=compute_75;-gencode;arch=compute_80,code=compute_80;-use_fast_math;;-mavx512f -mfma -fexcess-precision=fast -funroll-all-loops -fopenmp -O3 -DNDEBUG CUDA driver: 11.20 CUDA runtime: 11.10 Copy "},{"title":"Test simulation of Lysozyme","type":1,"pageTitle":"gromacs on GPUs","url":"blog/gromacs-gpu#test-simulation-of-lysozyme","content":"I used a tutorial from the gromacs website here to show it runs processes on GPU(s). The tutorial runs an MD simulation on a lysozyme but that's the extent of my study there. The commands below are a summary of the tutorial with a note that the genbox subcommand is now replaced by solvate. gmx pdb2gmx -f 1LYD.pdb -water tip3p gmx editconf -f conf.gro -bt dodecahedron -d 0.5 -o box.gro gmx solvate -cp box.gro -cs spc216.gro -p topol.top -o solvated.gro gmx trjconv -s solvated.gro -f solvated.gro -o solvated.pdb gmx grompp -f em.mdp -p topol.top -c solvated.gro -o em.tpr -maxwarn 3 Copy The final gromacs command below starts the fun, the documentation suggests it will automatically identify the GPUs available to send work to them. However, there are more explicit GPU arguments we encourage you to explore. gmx mdrun -v -deffnm em Copy You can ssh into the node you're using in a separate window to have a parallel nvidia-smi command run so we can monitor the load on the GPU(s). +-------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |===================================================================| | 0 N/A N/A 143353 C gmx 165MiB | | 1 N/A N/A 143353 C gmx 165MiB | | 2 N/A N/A 143353 C gmx 167MiB | | 3 N/A N/A 143353 C gmx 167MiB | | 4 N/A N/A 143353 C gmx 167MiB | | 5 N/A N/A 143353 C gmx 167MiB | | 6 N/A N/A 143353 C gmx 167MiB | | 7 N/A N/A 143353 C gmx 165MiB | +-------------------------------------------------------------------+ Copy We can see a process occuping each GPU so it works! At least, gromacs uses GPUs...the GPUs themselves weren't stressed heavily and that requires the user to increase the number of rank processes and match that with available GPUs. You can do this by adding arguments to the gmx mdrun command but by default it did 2 ranks per GPU it detected, which is not a lot. "},{"title":"(Optional) Compile Notes","type":1,"pageTitle":"gromacs on GPUs","url":"blog/gromacs-gpu#optional-compile-notes","content":"You need CUDA11, GNU Compiler, and OpenBLAS library for the version I put together but I was focused on a proof-of-concept and not squeezing out every last drop of performance. There's a lot of further optimization to be done and that's left as an exercise for the reader: Try the Intel compiler and see if it provides further optimization for non-GPU parts of the workflow.Try other math libraries (e.g., MKL) and see if it speeds things up.Add in MPI support if you want to use multiple GPUs across multiple nodes.Add in modules (e.g., PLUMED).Other stuff I can't think of with compile flags [here]. "},{"title":"Download Source","type":1,"pageTitle":"gromacs on GPUs","url":"blog/gromacs-gpu#download-source","content":"From the login node I staged a folder in the modules directory. cd /sw/gromacs/2020.4-cuda11.1 Copy Grab regression tests. wget http://gerrit.gromacs.org/download/regressiontests-2020.4.tar.gz Copy Download gromacs-2020.4 [source]. wget ftp://ftp.gromacs.org/pub/gromacs/gromacs-2020.4.tar.gz Copy "},{"title":"Get a GPU and Code","type":1,"pageTitle":"gromacs on GPUs","url":"blog/gromacs-gpu#get-a-gpu-and-code","content":"I used the shared build-gpu node for an interactive session but if you are affiliated with a group that has their own you can use that instead. srun -p build-gpu --time=4:00:00 -n 4 --mem=20G --gpus=1 --pty $0 Copy Once you get a session with GPU (you can run nvidia-smi to confirm you see one). Extract regression tests. tar xvzf regressiontests-2020.4.tar.gz Copy Do the same for the gromacs code and enter the directory. tar xzvf gromacs-2020.4.tar.gz cd gromacs-2020.4 Copy "},{"title":"Pre-requisite Modules","type":1,"pageTitle":"gromacs on GPUs","url":"blog/gromacs-gpu#pre-requisite-modules","content":"Modules loaded individually for readability but you could load all modules in one command. Get a refresher on modules here. module load cmake/3.11.2 module load gcc/10.1.0 module load cuda/11.1.1-1 module load contrib/openblas/0.2.20 Copy "},{"title":"Compile","type":1,"pageTitle":"gromacs on GPUs","url":"blog/gromacs-gpu#compile","content":"I created a subdirectory within the source to compile. mkdir cuda11 cd cuda11 Copy Use cmake to create the Makefile. Note: if you copy-and-paste the cmake command below you will have to modify the paths referenced for your environment. cmake .. -DGMX_BUILD_OWN_FFTW=OFF -DREGRESSIONTEST_DOWNLOAD=OFF -DGMX_GPU=ON -DGMX_MPI=OFF -DCMAKE_INSTALL_PREFIX=/sw/gromacs/2020.4-cuda11.1 -DREGRESSIONTEST_PATH=/sw/gromacs/2020.4-cuda11.1/regressiontests-2020.4 -DCUDA_TOOLKIT_ROOT_DIR=/sw/cuda/11.1.1-1 Copy With the Makefile ready you can run make -j 4 and replace 4 with however many cores you have in your session then make install. I created the module file separately so you can load it with module load gromacs/2020.4-cuda11.1 and run the single gmx binary. "},{"title":"Hello world!","type":0,"sectionRef":"#","url":"blog/hello-world","content":"tl;dr (1) decomissioned a cluster, (2) got a bunch of GPUs for maching learning, (3) launched a cluster, and (4) new and improved documentation. 2020 has definitely been an eventful year but here on Team Hyak we've been trying to make the best of a bad situation (lemons out of lemonade and such). This year saw the decomissioning of the 1st generation Hyak cluster, ikt, and the soft launch of our 3rd generation Hyak cluster, klone. Our partnership with the Allen School and other departments across campus has enabled an explosion in on-campus GPU capacity for the current 2nd generation Hyak cluster, mox. This is all very exciting, machine learning is only going to get bigger. We realize whether you do your research on your laptop, Hyak, or the cloud that at the end of the day it's all just a computer and what matters is what you can actually do with it. Therefore, we are placing more emphasis on new and improved documentation (this website) and will be doing more regular research tutorials on Hyak throughout the coming year. We hope you have weathered the adversity 2020 brought upon everyone. It has been a tough year for sure, but may your 2021 be brighter and have improvements in store. The Hyak Team has lots of efforts in the works to benefit supporting your research and they will hit full stride in the coming year. This is one improvement we can all look forward to in 2021.","keywords":""},{"title":"Migrating from MOX to KLONE","type":0,"sectionRef":"#","url":"blog/mox-to-klone","content":"","keywords":""},{"title":"Login","type":1,"pageTitle":"Migrating from MOX to KLONE","url":"blog/mox-to-klone#login","content":"Logging in was previously to mox.hyak.uw.edu now it's klone.hyak.uw.edu.As a reminder login nodes are only to connect to the cluster, navigate the cluster file system, and submit jobs. This applies to both KLONE and MOX. Do not compile codes on the login node or run any programs that require significant compute (get a session with SLURM). "},{"title":"Data Transfer","type":1,"pageTitle":"Migrating from MOX to KLONE","url":"blog/mox-to-klone#data-transfer","content":"Only use the login node to transfer data on KLONE. On MOX you'd have used a build node or could have used the login node if it wasn't very computationally heavy. "},{"title":"Storage","type":1,"pageTitle":"Migrating from MOX to KLONE","url":"blog/mox-to-klone#storage","content":"The path to lab storage is still /gscratch/mylab on both KLONE and MOX. You'll need to copy over the data from MOX to KLONE you want to continue using.Home directories are still 10GB per user, same on both clusters.Scrubbed exists on KLONE just as it did on MOX at /gscratch/scrubbed this is a free-for-all space on both clusters where files are automatically deleted after 21 days. Some new benefits of the KLONE storage compared to MOX: There are snapshots for gscratch! Look inside the /gscratch/mylab/.snapshots folder for a copy of your lab folder once an hour, every hour, for 24 hours. This is not a backup copy nor a replacement for version management (e.g., git) but useful for retrieving recent versions or something accidentally deleted.It's faster! We've had reports of performance that's averaging a 30% speed up all else being equal, nothing you need to do aside from use KLONE instead of MOX.It's faster than fast! While KLONE storage is faster than MOX storage overall, gscratch on KLONE is further turbo charged with a NVMe flash based tier. NVMe flash is among the fastest storage mediums you can get and further differentiating benefit if you use gscratch vs scrubbed on KLONE. "},{"title":"Compute","type":1,"pageTitle":"Migrating from MOX to KLONE","url":"blog/mox-to-klone#compute","content":"When submitting a SLURM job, whether interactive (i.e., srun) or batch (i.e., sbatch) you'll want to first decide which account to use. This is the group you're part of. You can run the command groups to see your affiliated accounts and run hyakalloc to see all the resources (e.g., compute cores, memory, GPUs) used and available associated with each affiliated account.Then decide if you want to run this job to count under your resource allocation by submitting to the compute partition (i.e., -p compute) or if you want this job to use idle resources from other groups across the cluster using the checkpoint partition (i.e., -p ckpt). Non-standard partitions. Run sinfo to see the list of all possible partitions, this is only if your group contributed non-standard nodes (e.g., high memory, GPUs) and need to idenitify the appropriate partition names to get immediate use. Otherwise, you'd only be able to get them in a checkpoint capacity.There is no build node on KLONE. Get an interactive session under an existing account and partition combinatino you have access to. All nodes have internet now on KLONE. Do all data transfers to and from KLONE on the KLONE login nodes, the login nodes on KLONE have dual 40 Gbps uplinks to the internet. While the compute nodes on KLONE have internet routing now, they are bottlenecked at 1 Gbps so not suitable for optimal data transfer.  "},{"title":"Software","type":1,"pageTitle":"Migrating from MOX to KLONE","url":"blog/mox-to-klone#software","content":"Singularity containers work the same on both clusters, we encourage this when possible. Refer to our container documentation [link].Modules is updated to the latest versions of the most core parts that the HYAK team maintains (e.g., gcc, Intel, Matlab). Refresh yourself about modules [link].If neither Singularity nor existing modules works for you, you may have to re-compile your codes on KLONE. \"contrib\" modules works different now on KLONE vs MOX, please check out the details [link]. "},{"title":"Klone Soft Launch","type":0,"sectionRef":"#","url":"blog/klone","content":"","keywords":""},{"title":"February 25, 2021","type":1,"pageTitle":"Klone Soft Launch","url":"blog/klone#february-25-2021","content":"The UW research computing team celebrates the soft launch of project KLONE, the 3rd generation HYAK supercomputer. Welcome to those researchers invited to participate in the early access program 🥳 🎉 caution There will be weekly maintenance days on Tuesday during the soft launch period after which we will move back to our regular cadence of monthly maintenance windows. The user documentation [link] has been updated to reflect the changes and new features of KLONE but this will be an ongoing process. Compute# Soft launch with 1,920 compute cores over 48 nodes: 28 x mem1 nodes (192GB of memory each) in the compute partition,4 x mem2 nodes (384GB of memory each) in the compute-bigmem partition,16 x mem3 nodes (768GB of memory each) in the compute-hugemem partition. build nodes no longer exist on klone as they did on mox. All instances have the potential to be interactive and all have internet routing by default (even non-interactive jobs). Storage# gscratch on klone is 1.4PB total capacity with a new 500TB NVMe flash tier. Data tiering happens automagically, if you use a file frequently it will be moved to the faster storage.Storage quota is still charged back at the same rate ($10 / TB / month). Researchers receive 1TB per node purchased and contributed to klone. Data# gscratch is not backed up that is the responsibility of the researcher (e.g., LOLO, the cloud, external hard drive). Feel free to email us if you have any questions.While all nodes have internet access now, transfer data using the login nodes. Login nodes have full 2 x 40 Gbps bandwidth. If you transfer using a compute node interactive session you are limited to 1 x 1 Gbps connection. Software# modules works the same as it did on mox. This is an improved implementation called LMOD on klone compared to environment modules on mox.We provide the basic compilers (e.g., GNU, Intel) as modules.The HYAK team is encouraging a container first world (i.e., use Singularity). "},{"title":"March 3, 2021","type":1,"pageTitle":"Klone Soft Launch","url":"blog/klone#march-3-2021","content":"The updated total is 3,840 cores and 96 nodes on klone. Compute# Compute has doubled by adding another rack to klone, an additional 1,920 compute cores over 48 nodes: 44 x mem1 nodes (192GB of memory each) in the compute partition,2 x mem2 nodes (384GB of memory each) in the compute-bigmem partition,2 x mem3 nodes (768GB of memory each) in the compute-hugemem partition. Software# We created a module for cmake. "},{"title":"March 5, 2021","type":1,"pageTitle":"Klone Soft Launch","url":"blog/klone#march-5-2021","content":"Storage# Implemented usage_report.txt files in the base folder of /gscratch/yourlab/ that is updated once an hour to reflect both your block quota and inode capacity usage. This is similar to the gscratch experience on the MOX cluster. Website# We migrated our site from https://UWrc.github.io to its new home at https://hyak.uw.edu. "},{"title":"March 9, 2021","type":1,"pageTitle":"Klone Soft Launch","url":"blog/klone#march-9-2021","content":"Storage# Snapshots are here! We are piloting once an hour for 24 hours for every lab storage folder under /gscratch/. Check out the updated documentation here on how to access past snapshots. Software# We created more LMOD software modules: Matlab R2020b [docs]OpenMPI-4.1.0 "},{"title":"March 12, 2021","type":1,"pageTitle":"Klone Soft Launch","url":"blog/klone#march-12-2021","content":"LMOD software modules: Intel has bundled their software suite (e.g., compiler, MPI) as oneCLI and we created this module (i.e., module load intel/oneCLI).There is now a \"contrib\" framework for groups to store their shared codes separately from their /gscratch/labname/ data. You can get 100GB of storage to compile codes at /sw/contrib/labname-src/ and then put your LMOD module file in /sw/contrib/modulefiles/labname/. Your module would appear when anyone runs module avail. This is created upon request so if you'd like to opt-in your group please let us know. "},{"title":"April 13, 2021","type":1,"pageTitle":"Klone Soft Launch","url":"blog/klone#april-13-2021","content":"Things have been going steady the past week and changes are coming less frequently. We are now increasing time between maintenance periods on klone from weekly on Tuesdays to monthly and aligning it with the mox maintenance as the 2nd Tuesday of every month. That wraps up our klone soft launch blog updates here, other updates will appear on our HYAK users mailing list. Don't forget to subscribe, instructions on this page at the bottom. "},{"title":"Pytorch and CUDA11","type":0,"sectionRef":"#","url":"blog/pytorch-cuda11","content":"","keywords":""},{"title":"Installing Pytorch with CUDA11","type":1,"pageTitle":"Pytorch and CUDA11","url":"blog/pytorch-cuda11#installing-pytorch-with-cuda11","content":"Since this is now the latest and greatest on HYAK I've taken the opportunity to update the Python documentation on how to install Pytorch with CUDA11 support within a miniconda3 environment, check out the step-by-step here. "},{"title":"Reverse compatibility with CUDA10","type":1,"pageTitle":"Pytorch and CUDA11","url":"blog/pytorch-cuda11#reverse-compatibility-with-cuda10","content":"Before the January 12, 2021 cluster maintenance every GPU on HYAK had a driver with CUDA10 and all of your codes were previously compiled against it. To test that the GPU driver update to CUDA11 wouldn't impact the most popular machine learning libraries we are compiling Pytorch against our pre-maintenance CUDA10 and testing it against a GPU with the newer CUDA11 installed. conda create -p /gscratch/scrubbed/npho/pytorch-cuda10 python=3.8 -y Copy Activate your new pytorch-cuda10 environment: conda activate pytorch-cuda10 Copy The Pytorch website [www] has a nice getting started matrix that generates the requisite install commands against CUDA10.  The command shown above to copy-and-paste below: pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html Copy Now we can load the Python interpreter and confirm Pytorch is installed and the CUDA10 compiled library recognizes this GPU with CUDA11 [www]. (pytorch-cuda10) $ python3 Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.__version__ '1.7.1+cu101' >>> torch.cuda.is_available() True >>> Copy Success! Previously compiled libraries against CUDA10 from pre-January 12, 2021 maintenance times should still work on the GPUs now with CUDA11. However, if you want to use the full features of libraries that take advantage of newer capabilities in CUDA11 then you should definitely upgrade your libraries. "},{"title":"scRNA-seq","type":0,"sectionRef":"#","url":"blog/scrna-seq","content":"Over the past year we've done a bunch of outreach to labs within the genomics community here at the UW. Given my research background it's where I feel most comfortable and this type of research is underrepresented on the cluster. Given where the field is going it's inconceivable they aren't using Hyak or HPC in general! Those lab closet clusters and under bench servers gotta go, there is a better way and we'll show you how to do some scRNA-seq today 🙂 TODO","keywords":""},{"title":"Start Here","type":0,"sectionRef":"#","url":"docs/","content":"Welcome!","keywords":""},{"title":"Account Activation","type":0,"sectionRef":"#","url":"docs/account-activation","content":"Your UW Net ID has been enabled for access to the Hyak system. You must now subscribe to the service. After you subscribe, it may take up to an hour for your account to be fully provisioned. Go to uwnetid.washington.eduClick the \"Computing Services\" link on the leftClick the \"Hyak Server\" check box in the \"Inactive Services\" sectionClick the \"Subscribe >\" button at the bottom of the pageRead the notice and click the \"Finish\" button UW policy is that all services (of which HYAK is one) require 2 factor authentication (2FA) by default as a security posture. important You need 2FA to log onto any HYAK cluster. You can use a personal device (e.g., phone, tablet) or a land line as your two-factor token, please go to this 2FA page and ensure you have it enabled and configured. If you wish to use an older style single purpose token, visit this page and ask for a security token. tip Sign up for the HYAK mailing list here and stay up to date on system news! The HYAK mailing list is low volume (average < 1 email per month) and provides critical announcements in the event of a service issue, maintenance, or other updates.","keywords":""},{"title":"Account Creation","type":0,"sectionRef":"#","url":"docs/account-creation","content":"","keywords":""},{"title":"I have a PI and the PI contributed HYAK nodes.","type":1,"pageTitle":"Account Creation","url":"docs/account-creation#i-have-a-pi-and-the-pi-contributed-hyak-nodes","content":"Your PI can create a HYAK account for you by virtue of adding you to their group. Refer to the previous section about adding users to a group. "},{"title":"I have a PI and we have no HYAK nodes.","type":1,"pageTitle":"Account Creation","url":"docs/account-creation#i-have-a-pi-and-we-have-no-hyak-nodes","content":"You need to email us to get a free tier account. Please provide your UW netID. If your PI has purchased HYAK storage you will need them to follow the step above to be added to the group and have access to your shared data on the cluster after you're able to log in. "},{"title":"I'm a UW student and I have no PI.","type":1,"pageTitle":"Account Creation","url":"docs/account-creation#im-a-uw-student-and-i-have-no-pi","content":"If you are a UW student tech fee (STF) paying student, which is almost every UW student, you are eligible to join the Research Computing Club (RCC). The RCC has a pool of nodes on every HYAK cluster for you to use. You can join the RCC and thereby get a Hyak account created through them here and use their resource capacity on the cluster. Once your account is created you can resume to the next page with instructions on how to activate. "},{"title":"I am an external collaborator or I have no UW netID.","type":1,"pageTitle":"Account Creation","url":"docs/account-creation#i-am-an-external-collaborator-or-i-have-no-uw-netid","content":"Having a UW netID is a pre-requisite for any type of HYAK account creation. If you are an external collaborator of a current HYAK user and need a UW netID you will need to have your UW collaborator sponsor you for a netID. Please have them follow these instructions then once you have your netID, find yourself in one of the previous categories to be added to a group. Once you are added to a group you can resume with account activation on the next page. "},{"title":"Linking Markdown to Docusaurus","type":0,"sectionRef":"#","url":"docs/contribute/link-markdown","content":"","keywords":""},{"title":"Markdown Files","type":1,"pageTitle":"Linking Markdown to Docusaurus","url":"docs/contribute/link-markdown#markdown-files","content":"Basically all Markdown files go in the docs folder Each Markdown file should have at least id and title YAML headers: --- id: some-id # seems like the convention is to separate words with hyphens? title: Some Title --- ...rest of Markdown document... Copy  "},{"title":"Sidebar Configuration","type":1,"pageTitle":"Linking Markdown to Docusaurus","url":"docs/contribute/link-markdown#sidebar-configuration","content":"Add the id of the Markdown document to the correct category/make new category in sidebars.js sidebars.js should have a general structure like: module.exports = { sidebarName: { \"Some Category Name\": [ \"some-id\", \"another-id\", ... ], \"Another Category Name\": [ \"yet-another-id\", { // subcategory declaration type: 'category', label: 'Subcategory Name', items: [ 'subcategory-doc-id', ... ] } \"yet-yet-another-id\" ... ] } } Copy  Full description of possible headers (more info here): id: A unique document id. If this field is not present, the document's id will default to its file name (without the extension). (Please still explicitly include the id though! ) title: The title of your document. If this field is not present, the document's title will default to its id. (Also explicitly include the title too)hide_title: Whether to hide the title at the top of the doc. By default it is false.sidebar_label: The text shown in the document sidebar and in the next/previous button for this document. If this field is not present, the document's sidebar_label will default to its title.custom_edit_url: The URL for editing this document. If this field is not present, the document's edit URL will fall back to editUrl from options fields passed to docusaurus-plugin-content-docs.keywords: Keywords meta tag for the document page, for search engines.description: The description of your document, which will become the <meta name=\"description\" content=\"...\"/> and <meta property=\"og:description\" content=\"...\"/> in <head>, used by search engines. If this field is not present, it will default to the first line of the contents.image: Cover or thumbnail image that will be used when displaying the link to your post. More information about sidebars here. "},{"title":"Scheduling Jobs","type":0,"sectionRef":"#","url":"docs/compute/scheduling-jobs","content":"","keywords":""},{"title":"Interactive Nodes","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#interactive-nodes","content":"There are two types of interactive nodes. Compute nodes run computations but cannot connect to the internet. Build nodes are compute nodes that can connect to the Internet to get files and install packages from outside the mox ecosystem. "},{"title":"Obtaining Interactive Nodes","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#obtaining-interactive-nodes","content":"To get an interactive compute node with <size> GB of memory in your group partition called <group_name> for <time> hours, use: srun -p <group_name> --time=<time> --mem=<size>G --pty /bin/bash Copy Common acceptable time formats include hours:minutes:seconds, days-hours, and minutes. Example: [linj66@mox2 ~]$ srun -p stf --time=1:00:00 --mem=20G --pty /bin/bash [linj66@n2148 ~]$ Copy  To get an interactive compute node with <num_cores> cores, use: srun -p <group_name> -A <group_name> --nodes=1 \\ --ntasks-per-node=<num_cores> --time=<time> \\ --mem=<size>G --pty /bin/bash Copy  To get multiple interactive compute nodes with <num_nodes> as the number of nodes and <cores_per_node> as the number of cores, use: srun -p <group_name> -A <group_name> --nodes=<num_nodes> \\ --ntasks-per-node=<cores_per_node> --time=<time> \\ --mem=<size>G --pty /bin/bash Copy When this command runs, you will automatically enter into a session in one of the allocated nodes. To view the names of all your allocated nodes, use scontrol show hostnames. important If you are using an interactive node to run a parallel application such as Python multiprocessing, MPI, OpenMP etc. then the number given for the --ntasks-per-node option must match the number of processes used by your application.  If your group has an interactive node, use the option -p <group_name>-int like so: srun -p <group_name>-int -A <group_name> --time=<time> --mem=<size>G --pty /bin/bash Copy note --pty /bin/bash must be the last option given in above commandIf you do not obtain a build node with the specified --mem value, try smaller memory values For more details, read the srun man page. "},{"title":"Build Nodes","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#build-nodes","content":"Build nodes are allocated from the build group partition. To obtain a build node, execute srun with the option -p build. "},{"title":"Specifying Memory Size","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#specifying-memory-size","content":"It is important to use the --mem option to specify the memory allocation; otherwise the Slurm scheduler limits the memory allocation to a default value which is usually quite low. The value given to --mem should be smaller than the memory of the node as the operating system needs some. For 64GB nodes, use --mem=58GFor 128GB nodes, use --mem=120GFor 192GB nodes, use --mem=185GFor 256GB nodes, use --mem=248GFor 384GB nodes, use --mem=374GFor 512GB nodes, use --mem=500GFor 768GB nodes, use --mem=752GFor the knl nodes, use --mem=200G "},{"title":"Slurm Environment Variables","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#slurm-environment-variables","content":"When a job scheduled by Slurm begins, it needs to about how it was scheduled, what its working directory is, who submitted the job, the number of nodes and cores allocated to it, etc. This information is passed to Slurm via environment variables. Additionally, these environment variables are also used as default values by programs like mpirun. To view a node's Slurm environment variables, use export | grep SLURM. A comprehensive list of the environment variables Slurm sets for each job can be found at the end of the sbatch man page. "},{"title":"Batch Jobs","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#batch-jobs","content":""},{"title":"Single Node Batch Jobs","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#single-node-batch-jobs","content":"Below is a slurm script template. Submit a batch job from the mox login node by calling sbatch <script_name>.slurm. <script_name>.slurm !/bin/bash # JOB NAME SBATCH --job-name=<your_job_name> # ALLOCATION DEFINITION # The account and partition options should be the same # except in a few cases (e.g. ckpt queue, genpool queue) SBATCH --account=<group_name> SBATCH --partition=<group_name> # RESOURCES SBATCH --nodes=<num_nodes> # total number of nodes allocated SBATCH --ntasks-per-node=<cores_per_node> # cores per node # WALL TIME # Do not specify a wall time significantly more than your job needs # Common acceptable time formats: # hours:minutes:seconds e.g. 3:00:00 for 3 hours # minutes # days-hours SBATCH --time=<time> # MEMORY PER NODE # See above \"Specifying Memory Size\" for options SBATCH --mem=<size>G # e.g. --mem=100G for 100 GB of memory # WORKING DIRECTORY ENTRYPOINT # Specify the working directory for this job SBATCH --chdir=/gscratch/<group_name>/<net_id>/path/to/dir # Turn on email notifications SBATCH --mail-type=ALL SBATCH --mail-user=<your_email> # Export all environment variables to the batch job session SBATCH --export=all # Run the commands to run your program here # e.g. load modules, copy input.output files, run program, etc. <commands_to_run_your_program> Copy "},{"title":"Multiple Node Batch Jobs","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#multiple-node-batch-jobs","content":"If your batch job is using multiple nodes, your program should also know how to use all the nodes (e.g. your program is an MPI program). The value given for --nodes must be less than or equal to the total number of nodes owned by your group. The value given for --ntasks-per-node should be either 28 for older mox nodes or 40 for newer nodes. Do not increase these values. You can decrease these values if your program is running out of memory on a node. SBATCH --nodes=4 SBATCH --ntasks-per-node=28 # OR SBATCH --ntasks-per-node=40 Copy "},{"title":"Self-Limiting Your Number of Running Jobs","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#self-limiting-your-number-of-running-jobs","content":"note This feature is not enabled on the ckpt partition At times you may wish to self-limit the number of jobs that will be run simultaneously in order to leave nodes in your group's partition for other group members. To achieve this, you can add SBATCH --qos=MaxJobs<n> where n is a number between 1 and 10 to tell the job scheduler to allow only n jobs running with the option --qos=MaxJobs<n>. However, any other jobs without this option set are not limited and jobs with a different value of n are gated separately. "},{"title":"Common Slurm Error Messages","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#common-slurm-error-messages","content":"slurmstepd: error: Exceeded job memory limit: your program uses more memory than you allotted during node creation and it has run out of memory. Get a node with more memory and try again.(ReqNodeNotAvail, UnavailableNodes:n[<node numbers list>]: your node will not expire (and might be running one of your jobs) before the next scheduled maintenance day. Either get a node with a shorter --time duration or wait until after the maintenance has been completed.Unable to allocate resources: Invalid account or account/partition combination specified: you used -p <group_name> -A <group_name> and you do not belong to that group. "},{"title":"Utility Commands","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#utility-commands","content":"With <net_id> as your UW NetID and <group_name> as your Hyak group partition name, and <job_id> as an individual job ID: sinfo is used to view information about mox nodes and partitions. Use sinfo -p <group_name> to view information about your group's partition or allocation.squeue is used to view information about jobs located in the scheduling queue. Use squeue -p <group_name> to view information about your group's nodes. Use squeue -u <net_id> to view your jobs.scancel is used to cancel jobs. Use scancel <job_id> to cancel a job with the given job ID, or use scancel -u <net_id> to cancel all of your jobs.sstat displays status information of a running job pertaining to CPU, Task, Node, Resident Set Size (RSS), and Virtual Memory (VM) statistics. Read the man page for a comprehensive list of format options. sacct displays information about completed jobs. Read the man page for a comprehensive list of format options.sreport generates reports about job usage and cluster utilization from Slurm accounting (sacct) data. For example, to get historical usage the group <group_name> in March 2020, use sreport cluster UserUtilizationByAccount Start=2020-03-01 End=2020-03-31 Accounts=<group_name>. "},{"title":"FOR ADVANCED USERS ONLY: salloc","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#for-advanced-users-only-salloc","content":"warning Do not use salloc unless you have a specific reason. To get nodes for interactive use: salloc -N <num_nodes> -p <group_name> -A <group_name> --time=<time> --mem=<size>G Copy When this command runs, you will have been allocated num_nodes nodes but you will still be on the mox login node. Use srun <command> to run commands on all allocated nodes. Use scontrol show hostnames to get the hostnames of your allocated nodes. Once you have the hostnames, you can ssh to them using ssh <hostname> and then use them for your work (e.g. Apache Spark, Hadoop, etc.) Example: [linj66@mox2 ~]$ salloc -N 2 -p stf -A stf --time=5 --mem=5G salloc: Pending job allocation 2620960 salloc: job 2620960 queued and waiting for resources salloc: job 2620960 has been allocated resources salloc: Granted job allocation 2620960 salloc: Waiting for resource configuration salloc: Nodes n[2148-2149] are ready for job [linj66@mox2 ~]$ srun echo \"test\" test test [linj66@mox2 ~]$ scontrol show hostnames n2148 n2149 [linj66@mox2 ~]$ ssh n2148 Warning: Permanently added 'n2148,10.64.56.248' (ECDSA) to the list of known hosts. [linj66@n2148 ~]$ Copy "},{"title":"Man Pages","type":1,"pageTitle":"Scheduling Jobs","url":"docs/compute/scheduling-jobs#man-pages","content":"All of these man pages can also be viewed on mox by running man <command>. sacctsallocsbatchscancelscontrolsinfosqueuesreportsrunsstat "},{"title":"Powered by MDX","type":0,"sectionRef":"#","url":"docs/contribute/mdx","content":"You can write JSX and use React components within your Markdown thanks to MDX. Docusaurus green and Facebook blue are my favorite colors. I can write Markdown alongside my JSX!","keywords":""},{"title":"Join a Group","type":0,"sectionRef":"#","url":"docs/join-group","content":"","keywords":""},{"title":"How do you become a group owner?","type":1,"pageTitle":"Join a Group","url":"docs/join-group#how-do-you-become-a-group-owner","content":"Groups are tied to resources, your options are to contribute a node or have a dedicated lab storage space on a HYAK cluster. Once your group is created the group owner can add or remove users by their netIDs and those users will get access to those node(s) and / or associated storage. "},{"title":"How does a group owner add or remove a user?","type":1,"pageTitle":"Join a Group","url":"docs/join-group#how-does-a-group-owner-add-or-remove-a-user","content":"A group owner, usually the PI or designated staff (e.g., lab manager), should take the following steps: Log into groups.uw.eduClick \"My groups\" in the top menu barGo to u_hyak_mylab where mylab is your groups' nameClick \"Membership\" just underneath the top menu barA field appears to \"Add members\" by netID. Enter multiple netIDs one per line or separated by commas. NOTE: This is also how you remove members from your group when the time comes (under the \"Remove members\" field).Click \"Do it\" caution These netIDs now have access to your nodes and lab data on HYAK once they log into the cluster. "},{"title":"Markdown Guide","type":0,"sectionRef":"#","url":"docs/contribute/markdown-guide","content":"","keywords":""},{"title":"Markdown Syntax","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#markdown-syntax","content":"To serve as an example page when styling markdown based Docusaurus sites. "},{"title":"Headers","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#headers","content":"H1 - Create the best documentation# "},{"title":"H2 - Create the best documentation","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#h2---create-the-best-documentation","content":""},{"title":"H3 - Create the best documentation","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#h3---create-the-best-documentation","content":"H4 - Create the best documentation# H5 - Create the best documentation# H6 - Create the best documentation#  "},{"title":"Emphasis","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#emphasis","content":"Emphasis, aka italics, with asterisks or underscores. Strong emphasis, aka bold, with asterisks or underscores. Combined emphasis with asterisks and underscores. Strikethrough uses two tildes. Scratch this.  "},{"title":"Lists","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#lists","content":"First ordered list itemAnother item ⋅⋅* Unordered sub-list.Actual numbers don't matter, just that it's a number ⋅⋅1. Ordered sub-listAnd another item. ⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown). ⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) Unordered list can use asterisks Or minuses Or pluses  "},{"title":"Links","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#links","content":"I'm an inline-style link I'm an inline-style link with title I'm a reference-style link I'm a relative reference to a repository file You can use numbers for reference-style link definitions Or leave it empty and use the link text itself. URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later.  "},{"title":"Images","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#images","content":"Here's our logo (hover to see the title text): Inline-style:  Reference-style:   "},{"title":"Code","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#code","content":"var s = 'JavaScript syntax highlighting'; alert(s); Copy s = \"Python syntax highlighting\" print(s) Copy No language indicated, so no syntax highlighting. But let's throw in a <b>tag</b>. Copy function highlightMe() { console.log('This line can be highlighted!'); } Copy  "},{"title":"Tables","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#tables","content":"Colons can be used to align columns. Tables\tAre\tCoolcol 3 is\tright-aligned\t\\$1600 col 2 is\tcentered\t\\$12 zebra stripes\tare neat\t\\$1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown\tLess\tPrettyStill\trenders\tnicely 1\t2\t3  "},{"title":"Blockquotes","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#blockquotes","content":"Blockquotes are very handy in email to emulate reply text. This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote.  "},{"title":"Inline HTML","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#inline-html","content":"Definition list Is something people use sometimes. Markdown in HTML Does *not* work **very** well. Use HTML tags.  "},{"title":"Line Breaks","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#line-breaks","content":"Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the same paragraph.  "},{"title":"Admonitions","type":1,"pageTitle":"Markdown Guide","url":"docs/contribute/markdown-guide#admonitions","content":"note This is a note tip This is a tip important This is important caution This is a caution warning This is a warning "},{"title":"Getting Set Up","type":0,"sectionRef":"#","url":"docs/setup","content":"Probably have something about how to register with UW RCC and getting Hyak and Lolo up and what not here.","keywords":""},{"title":"SSH","type":0,"sectionRef":"#","url":"docs/setup/ssh","content":"","keywords":""},{"title":"Logging into KLONE","type":1,"pageTitle":"SSH","url":"docs/setup/ssh#logging-into-klone","content":"ssh netID@klone.hyak.uw.edu Copy You log into the klone.hyak.uw.edu cluster above at the terminal using your netID. You will be prompted for your password and 2FA (DUO) authentication. We don't allow ssh keys to the login node since it would be bypassing one of the factors (of 2-factor authentication). "},{"title":"Intracluster SSH Keys","type":1,"pageTitle":"SSH","url":"docs/setup/ssh#intracluster-ssh-keys","content":"Once you're logged into the login node (e.g., klone-login01 or klone-login02) you'll use this as a host to submit jobs, transfer data, or navigate your environment. However, you may find yourself needing to either submit multi-node (i.e., MPI) jobs or log into a node after you have a job running there to check its progress. For this you need SSH keys set up for intra cluster access. You can generate an SSH key from the login node using the command below. ssh-keygen -C klone -t rsa -b 2048 -f ~/.ssh/id_rsa -q -N \"\" Copy The command creates a 2048-bit RSA key with \"klone\" in the comment field. You'll want to also add it to your authorized_keys file using the command below. cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys Copy It also ensures the file has appropriate permissions. "},{"title":"Start Here","type":0,"sectionRef":"#","url":"docs/setup/index","content":"Once your account is activated and provisioned you're ready to login. There are many interfaces to the cluster from IDE to CLI.","keywords":""},{"title":"Visual Studio Code","type":0,"sectionRef":"#","url":"docs/setup/vscode","content":"Using the Visual Studio Code (VSCode) ssh module you can use your local environment.","keywords":""},{"title":"Archive on LOLO","type":0,"sectionRef":"#","url":"docs/storage/archive","content":"","keywords":""},{"title":"What is LOLO Tape?","type":1,"pageTitle":"Archive on LOLO","url":"docs/storage/archive#what-is-lolo-tape","content":"LOLO is the UW's archive solution, it is an LTO-8 or \"tape\" based platform. note LOLO costs $4 / TB / month. "},{"title":"How do I get LOLO capacity?","type":1,"pageTitle":"Archive on LOLO","url":"docs/storage/archive#how-do-i-get-lolo-capacity","content":"Go to https://uwnetid.washington.edu/manage/Click the \"Computing Services\" link on the leftClick the \"Lolo Server\" check box in the \"Inactive Services\" section.Click the \"Subscribe >\" button at the bottom of the page.Read the notice and click the \"Finish\" button. "},{"title":"How to back up to LOLO?","type":1,"pageTitle":"Archive on LOLO","url":"docs/storage/archive#how-to-back-up-to-lolo","content":"TODO "},{"title":"Google Drive","type":1,"pageTitle":"Archive on LOLO","url":"docs/storage/archive#google-drive","content":"caution Update: This is no longer \"unlimited\" and data caps are set to go into effect in 2022. The UW has a relationship with Google for cloud services (e.g., Gmail, Google Drive). Google Drive has a reputation for \"unlimited\" data storage. It's possible you can use this as one place to store your data but we leave it to the user to perform their own due diligence on the implications of this. Some labs at the UW make use of Google Drive in this manner so we thought we'd make note of it here but we provide no support for it. "},{"title":"3-2-1 Policy","type":0,"sectionRef":"#","url":"docs/storage/best-practice","content":"Storage on every HYAK cluster is physically separate. It is best practice on every supercomputer that storage live on its own and is high-performance to handle the bandwidth I/O and read/write operations required by so many compute nodes attached to it. These are typically parallel file systems (e.g., GPFS, Lustre, BeeGFS). This storage system is then mounted (i.e., accessible) from every compute node of the cluster. Each HYAK cluster (e.g., klone, mox) has its own separate.","keywords":""},{"title":"Storage on HYAK","type":0,"sectionRef":"#","url":"docs/storage/gscratch","content":"","keywords":""},{"title":"User Home Directory","type":1,"pageTitle":"Storage on HYAK","url":"docs/storage/gscratch#user-home-directory","content":"10 GB, only yours, everyone has one. Each users' home directory is located at the folder path /mmfs1/home/netID on KLONE and /usr/lusers/netID or /gscratch/home/netID on MOX where netID is your UW netID. You are placed here by default when you log into MOX. You can always refer to it using the usual Linux shortcuts of $HOME or ~. note Your home directory quota is 10 GB or 250,000 files. You can check your live home directory usage using the following command. mmlsquota --block-size G gscratch:home Copy Ideally you only keep personal code bases or smaller data sets here. This quota can not be changed, if you need more data one of the other storage spots on gscratch are better suited. "},{"title":"Group or Lab Directories","type":1,"pageTitle":"Storage on HYAK","url":"docs/storage/gscratch#group-or-lab-directories","content":"Shared lab storage at $10 / TB / month.NVMe flash tier on KLONE.Hourly snapshots. If you run the groups command you'll see what groups you are a member of. For example, one of my groups is stf, which means I'm a member of the \"stf\" group (i.e., the Research Computing Club). Whatever groups you are seeing here you can access your lab storage at /gscratch/mylab/ where mylab is any group you're a member of. In this example that means I have access to the /gscratch/stf/ and only members of the stf group have access to this folder. Please note, on MOX the group names have a hyak prefix. For example, stf will appear as hyak-stf. Your lab gets 1 TB per node that your group has contributed to KLONE (or 4 TB per node in the case of a GPU node). tip Your lab quota can be increased for $10 / TB / month. Your lab storage quota can be increased (or decreased) in 1 TB granularity and adjusted on a month-to-month basis as your needs require. If you hit file (i.e., inode) limits, email us and we can increase those limits for no additional cost if your workflows warrant. important Check group quotas and current use by looking at the /gscratch/mylab/usage_report.txt file. Snapshots are done once an hour for 24 hours on every /gscratch/mylab/ folder on KLONE. SNAPSHOTS ARE NOT BACKUP! If you need to recover something navigate to the base directory of your lab folder in gscratch and look in the .snapshots folder like below. You can navigate to any point in time there is a snapshot and copy back out any file that existed in the recent past. $ ls -alh /gscratch/stf/.snapshots total 15K dr-xr-xr-x 2 root root 8.0K Feb 13 14:02 . drwxrws--- 5 root stf 512 Mar 9 14:57 .. drwxrws--- 3 root stf 512 Mar 8 20:22 @GMT-2021.03.09-17.03.01 drwxrws--- 3 root stf 512 Mar 8 20:22 @GMT-2021.03.09-17.16.01 drwxrws--- 3 root stf 512 Mar 8 20:22 @GMT-2021.03.09-18.00.01 drwxrws--- 4 root stf 512 Mar 9 10:05 @GMT-2021.03.09-19.00.01 drwxrws--- 4 root stf 512 Mar 9 10:05 @GMT-2021.03.09-20.00.01 drwxrws--- 4 root stf 512 Mar 9 10:05 @GMT-2021.03.09-21.00.01 drwxrws--- 4 root stf 512 Mar 9 10:05 @GMT-2021.03.09-22.00.01 drwxrws--- 5 root stf 512 Mar 9 14:57 @GMT-2021.03.09-23.00.01 drwxrws--- 5 root stf 512 Mar 9 14:57 @GMT-2021.03.10-00.00.01 drwxrws--- 5 root stf 512 Mar 9 14:57 @GMT-2021.03.10-01.00.01 drwxrws--- 5 root stf 512 Mar 9 14:57 @GMT-2021.03.10-02.00.01 drwxrws--- 5 root stf 512 Mar 9 14:57 @GMT-2021.03.10-03.00.01 drwxrws--- 5 root stf 512 Mar 9 14:57 @GMT-2021.03.10-04.00.01 $ Copy "},{"title":"Scrubbed","type":1,"pageTitle":"Storage on HYAK","url":"docs/storage/gscratch#scrubbed","content":"Free to use but files auto-deleted beyond 21 days.Slower than gscratch lab directories.No snapshots. If you need space but only temporarily (i.e., less than 3 weeks) then you can make use of the scrubbed folder. The scrubbed folder lives at /gscratch/scrubbed/ and anything underneath this folder is a free-for-all space. You can create a folder for yourself and do whatever you need subject to system constraints but note there is a purge policy where any file not accessed for 21 days (i.e., 3 weeks) is automatically deleted. This is to provide a useful (and free) flex capacity for any research group that needs it and can work within these policy restraints. However, we encourage users who need a more persistent storage location to purchase gscratch. caution AUTO-DELETE: Files not accessed for 3 weeks (i.e., 21 days) in scrubbed will automatically be deleted. Consider purchasing gscratch storage if you want a more persistent storage location. Starting with the KLONE cluster there are additional differentiating factors beyond the auto-delete policy, namely that all read and writes here will only stay on spinning disk. gscratch on KLONE has access to a tiering engine that auto writes to a performant NVMe flash tier so scrubbed will be slower than paid for gscratch on KLONE. On MOX there is no additional performance distinction for scrubbed compared to gscratch. Please note the scrubbed space is completely open so use Linux group changes and modifications to restrict access as appropriate. caution PRIVACY: Writes are public by default, it is the responsibility of the individual researcher to lock down anything they wish to use in scrubbed. "},{"title":"Start Here","type":0,"sectionRef":"#","url":"docs/storage/data","content":"","keywords":""},{"title":"What is storage for a supercomputer?","type":1,"pageTitle":"Start Here","url":"docs/storage/data#what-is-storage-for-a-supercomputer","content":"Storage on every HYAK cluster is physically separate. It is best practice on every supercomputer that storage live as its own infrastructure to be high-performance and able to handle the bandwidth I/O and read/write operations required by so many compute nodes attached to it. These are typically parallel file systems (e.g., GPFS, Lustre, BeeGFS). Storage systems are mounted (i.e., accessible) from every compute node of the cluster. Each HYAK cluster (e.g., klone, mox) has its own separate parallel file system. The storage attached to each HYAK cluster has its own policies, hierachy, etc. Please refer to their respective pages for more information. warning Cluster storage is not backed up! While our storage systems have a track record of stability, it is important to note that STORAGE IS NOT BACKED UP by default. It is the responsibility of the user that in the event of an incident you have a place and plan to restore their data. We provide a complementary archive service that is appropriate for this and other solutions exist. "},{"title":"3-2-1 Policy","type":1,"pageTitle":"Start Here","url":"docs/storage/data#3-2-1-policy","content":"Your data is precious, in some cases completely irreplacable. The research computing team encourages the use of the widely accepted 3-2-1 backup strategy. tip 3-2-1 is not a HYAK thing, it's a general IT best practice [Backblaze] [Acronis] [Networkworld]. The 3-2-1 backup policy suggests 3 copies of your data on 2 different types of storage media of which 1 copy is off-site. If you use both gscratch and LOLO then you are already adhering to this best practice, which is why it was designed this way. One copy resides in gscratch on our parallel file system and if you archive your data to LOLO, two additional copies are created (it does automatic duplication with one copy on UW-Seattle campus and another copy in eastern Washington). gscratch consists of spinning disk hard drives as a storage medium while LOLO is a tape-based storage medium. LOLO does one of its automatic duplication copies to a geographically remote data center in eastern Washington. caution You have to copy your data to LOLO to be 3-2-1 compliant or to use your own archive solution, it does not happen automatically. "},{"title":"Compilers","type":0,"sectionRef":"#","url":"docs/tools/compilers","content":"","keywords":""},{"title":"Data Transfer","type":0,"sectionRef":"#","url":"docs/storage/transfer","content":"Storage on every HYAK cluster is physically separate. It is best practice on every supercomputer that storage live on its own and is high-performance to handle the bandwidth I/O and read/write operations required by so many compute nodes attached to it. These are typically parallel file systems (e.g., GPFS, Lustre, BeeGFS). This storage system is then mounted (i.e., accessible) from every compute node of the cluster. Each HYAK cluster (e.g., klone, mox) has its own separate.","keywords":""},{"title":"GNU Compiler","type":1,"pageTitle":"Compilers","url":"docs/tools/compilers#gnu-compiler","content":"The latest GNU compiler provided as a module is version 10.2 [www]. This was built with the --enable-languages=c,c++,fortran flag. module load gcc/10.2.0 Copy There are older GNU compiler versions and combinations with MPI support. Additional (not listed below) modules prefixed with \"contrib\" are community provided and maintained. $ module avail gcc ----- /sw/klone ----- gcc/9.3.0 gcc/10.2.0 (D) $ Copy "},{"title":"Intel Compiler","type":1,"pageTitle":"Compilers","url":"docs/tools/compilers#intel-compiler","content":"The latest Intel compiler is the 2021 version, this module comes bundled with the entire oneAPI suite (e.g., Intel MPI). module load intel/oneAPI/2021.1.1 Copy "},{"title":"Jupyter Notebooks","type":0,"sectionRef":"#","url":"docs/tools/jupyter","content":"TODO","keywords":""},{"title":"Matlab","type":0,"sectionRef":"#","url":"docs/tools/matlab","content":"\"MATLAB is a proprietary multi-paradigm programming language and numeric computing environment developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages.\" [Wikipedia] The latest Matlab version on KLONE is R2020b. You can use LMOD [www] to load the module then run the binary, be sure to use the -nodisplay flag unless you enabled X11 forwarding to get the GUI. n3000:~ $ module load matlab n3000:~ $ matlab -nodisplay < M A T L A B (R) > Copyright 1984-2020 The MathWorks, Inc. R2020b Update 4 (9.9.0.1570001) 64-bit (glnxa64) January 7, 2021 To get started, type doc. For product information, visit www.mathworks.com. >> Copy","keywords":""},{"title":"Singularity and Docker","type":0,"sectionRef":"#","url":"docs/tools/containers","content":"","keywords":""},{"title":"Singularity","type":1,"pageTitle":"Singularity and Docker","url":"docs/tools/containers#singularity","content":"The official Singularity documentation [www] is the best source. "},{"title":"Ubuntu apt-get Example","type":1,"pageTitle":"Singularity and Docker","url":"docs/tools/containers#ubuntu-apt-get-example","content":"Let's say that you want to use git and the current version of git on HYAK is 1.8.3.1 as shown below. $ which git /usr/bin/git $ git --version git version 1.8.3.1 $ Copy Let's say you want a newer version AND you also want it running on Ubuntu for some reason. Here we'll walk you through installing the latest git binary using apt repositories for Ubuntu 16.04 [www] or \"Xenial Xerus\". Get an interactive session using some variant of the below command. srun -A mygroup -p compute --time=1:00:00 -n 2 --mem=10G --pty $0 Copy Load the Singularity module. module load singularity Copy Create a Singularity definition file. Mine is below called tools.def to install the latest curl and git binaries from the Ubuntu repositories. Bootstrap: docker From: ubuntu:16.04 %post apt -y update apt -y install curl git Copy Built a Singularity container from its definition file. The generated SIF file is your portable container. Note the definition file should be executable or you can switch the reference to file to ./tools.def if you're in the same working directory or provide the absolute path. singularity build --fakeroot tools.sif tools.def Copy Run the binary. You'll do this through the singularity executable to distinguish it from the git binary in the main operation system. $ singularity exec tools.sif git --version git version 2.7.4 $ Copy Notice the git version here is newer than the original we started with. Success! "},{"title":"App Stores","type":1,"pageTitle":"Singularity and Docker","url":"docs/tools/containers#app-stores","content":"If you followed the tutorial above you should be able to install anything you want but why re-create the wheel? There is a large developer community out there that maintains a majority of the most common scientific applications. "},{"title":"Sylabs.io Cloud Library","type":1,"pageTitle":"Singularity and Docker","url":"docs/tools/containers#sylabsio-cloud-library","content":" The largest collection of native Singularity containers can be found at the Sylabs.io Cloud Container Library [www]. This would be the ideal first place to look for containers built by others since it is maintained by the creators of Singularity and provides the native container format. "},{"title":"Docker Hub","type":1,"pageTitle":"Singularity and Docker","url":"docs/tools/containers#docker-hub","content":" The biggest collection of Docker images is from Docker Hub [www]. Let's say Docker Hub tells you the pull command for the container you want is docker pull gcc:11.1.0-bullseye. To have Singularity grab this Docker container and convert it to a Singularity container you'd modify the command to be singularity pull docker://gcc:11.1.0-bullseye. "},{"title":"NVIDIA GPU Cloud (NGC)","type":1,"pageTitle":"Singularity and Docker","url":"docs/tools/containers#nvidia-gpu-cloud-ngc","content":" A container registry that specializes in common GPU accelerated applications or GPU software development tools is provided by NVIDIA called the NVIDIA GPU Cloud (NGC) [www]. For example, you might want to use a PyTorch container optimized for NVIDIA GPUs as seen below.  Depending on the NGC container, it might have directions on the exact pull command for Singularity. If it does not work be sure to prepend their pull location with docker:// since these are native Docker containers that need to be converted to Singularity. The example above provides a Docker pull command for PyTorch but in this case you'd modify it similarly as if you got it from Docker Hub from docker pull nvcr.io/nvidia/pytorch:21.05-py3 to singularity pull docker://nvcr.io/nvidia/pytorch:21.05-py3. "},{"title":"NGC API Keys","type":1,"pageTitle":"Singularity and Docker","url":"docs/tools/containers#ngc-api-keys","content":"In rare occasions a container in the NGC app store is going to require that you have an API. This is the only reason you'd need to register for a user account with NGC. Once you have an NGC account and are logged in, at the top right the pull down menu select \"Setup\" and there's an option to \"Get API Key\". Save that string of text. You only have to register your API key once but load the ngc module (i.e., module load ngc) and run ngc config set which will prompt you for your API key. It's fine to select \"ascii\" as an option. Your API key will be stored under a .ngc/config file in your home directory. $ ngc config current +-------------+----------------------------------------------------------+--------------------+ | key | value | source | +-------------+----------------------------------------------------------+--------------------+ | apikey | ******************************************************** | user settings file | | | ************************YTc3 | | | format_type | ascii | user settings file | +-------------+----------------------------------------------------------+--------------------+ $ Copy "},{"title":"Modules","type":0,"sectionRef":"#","url":"docs/tools/modules","content":"","keywords":""},{"title":"Basics","type":1,"pageTitle":"Modules","url":"docs/tools/modules#basics","content":"Please refer to the cluster specific sections on KLONE [link] and MOX [link] below for more details on creating your own modules. "},{"title":"What software is available?","type":1,"pageTitle":"Modules","url":"docs/tools/modules#what-software-is-available","content":"module avail Copy The research computing team will maintain most of the core modules for building software, this includes GNU compilers (e.g., gcc, g++, gfortran) or their Intel compiler equivalents as well as select MPI libraries. There is a larger list of modules maintained by the broader HYAK community that appears when you run this command. Community created or \"contrib\" modules are provided as is. Community modules on KLONE are separated into a lower section and within the lower section each module is further prefixed by the respective group that created the module. All modules appear together when you run this command on MOX but the community provided modules appear with a \"contrib\" prefix. tip The HYAK team encourages the use of Singularity to better promote computational portability and reproducibility. You can read more about Singularity [link] after loading its module. "},{"title":"What modules do I currently have loaded?","type":1,"pageTitle":"Modules","url":"docs/tools/modules#what-modules-do-i-currently-have-loaded","content":"module list Copy "},{"title":"How to (un)load a software?","type":1,"pageTitle":"Modules","url":"docs/tools/modules#how-to-unload-a-software","content":"Replace \"software\" below with a specific module you know exists or identified via module avail above. module load <software> Copy Conversely, you can unload a specific module. module unload <software> Copy You can unload every module you might have loaded. module purge Copy "},{"title":"KLONE","type":1,"pageTitle":"Modules","url":"docs/tools/modules#klone","content":"The KLONE cluster uses the more feature-rich LMOD implementation of modules. You're welcome to email us if you have any questions about modulefile creation on KLONE. "},{"title":"LMOD","type":1,"pageTitle":"Modules","url":"docs/tools/modules#lmod","content":" LMOD [documentation] [project page] is an upgraded implementation of environment modules created by the Texas Advanced Computing Center (TACC) at the University of Texas. "},{"title":"How do I create personal LMOD modules on KLONE?","type":1,"pageTitle":"Modules","url":"docs/tools/modules#how-do-i-create-personal-lmod-modules-on-klone","content":"This advanced user documentation page from the LMOD developers walks you through this [link]. You need to compile your code separately first. In short, you provide a command directing it to the folder with your collection of module files: module use /path/to/personal/modulefiles Copy In this case you'll likely use a sub-directory under your lab's /gscratch folder or your home directory and create individual folders with independent software packages. Once you have code compiled a modulefile needs to be created for each software package you installed, there are some examples from basic to advanced [link]. "},{"title":"How do I create shared LMOD modules on KLONE?","type":1,"pageTitle":"Modules","url":"docs/tools/modules#how-do-i-create-shared-lmod-modules-on-klone","content":"Each group has a special folder for installing codes that are intended to be shared for all KLONE users. Each folder here gets a 100GB block quota and 160,000 inode quota at /sw/contrib/mylab-src where \"mylab\" is your account affiliation. We can raise these limits if specific code compiles require, however, in our experience the default quotas are sufficient for all but the most rare cases. You place your modulefiles in /sw/contrib/modulefiles/mylab and when anyone runs module avail it will now appear in the \"contrib\" section in the lower half. Note the prefix is automatically tagged to your group name for you to more easily identify the ones you contributed (and likely will use most regularly). "},{"title":"MOX","type":1,"pageTitle":"Modules","url":"docs/tools/modules#mox","content":"The MOX cluster uses an simpler implementation of modules called environment modules. You're welcome to email us if you have any questions about modulefile creation on MOX. "},{"title":"Environment Modules","type":1,"pageTitle":"Modules","url":"docs/tools/modules#environment-modules","content":" Environment modules [documentation] [Wikipedia] has a long development history going back to the 1990's. It's still in use today due to its simplicity and ease of deployment for cluster administrators and end users alike. "},{"title":"How do I create my own environment module on MOX?","type":1,"pageTitle":"Modules","url":"docs/tools/modules#how-do-i-create-my-own-environment-module-on-mox","content":"Compile your code under /sw/contrib/ then create a modulefile under /sw/modules-1.775/modulefiles/contrib/ and it will appear when you run module avail. There are existing modulefiles there you can use as a template for creating your own. caution Any contrib modules on MOX are provided and maintained by the local research community. Since no one except the original authors can vouch for the software supply chain provenance, anything under contrib is made publicly available as-is without any support, warranty, or guarantees. "},{"title":"Python","type":0,"sectionRef":"#","url":"docs/tools/python","content":"","keywords":""},{"title":"Miniconda3","type":1,"pageTitle":"Python","url":"docs/tools/python#miniconda3","content":"Miniconda3 is a leaner deployment of Python3 versus the more widely known Anaconda3 variant. While both are functionally equivalent, we encourage Miniconda3 vs Anaconda3 due to the reduced number of inodes (i.e., files) it creates. We provide a summarized version below to get started but more elaborate instructions directly from anaconda [www]. "},{"title":"Install","type":1,"pageTitle":"Python","url":"docs/tools/python#install","content":"Download the latest miniconda3 version. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Copy Install miniconda3 to your home directory. If you use large or multiple virtual environments you'll be better off specifying a lab directory or elsewhere due to the (inode and capacity) limits of home directories. bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3 Copy "},{"title":"Configure","type":1,"pageTitle":"Python","url":"docs/tools/python#configure","content":"You'll need to initialize your shell. For most people (i.e., unless you actively changed it) should be using the cluster default of bash. If use zsh, fish, or any other terminal then you'll need to swap out your terminal as appropriate. conda init bash Copy Optional: your terminal prompt will show (base) to indicate miniconda3 is active but it takes up a lot of screen real estate so I like to hide it until I'm actively using Python. The next command keeps miniconda3 deactivated until you call it. conda config --set auto_activate_base false Copy If you skip running the above command then miniconda3 will always be active. note If you do use the command above then you will need to run conda activate before each use and you can unload it with conda deactivate. "},{"title":"Environments","type":1,"pageTitle":"Python","url":"docs/tools/python#environments","content":"We'll assume you're using miniconda3 environments here for this walk through. It's generalizable to any Python environment but we will use an install of Pytorch against CUDA11 as the example since it is one of the most popular Python libraries used on HYAK. First create an environment, I put it in my scrubbed directory but you can put it anywhere (preferably your lab directory if you have one). We discourage using your home directory as you will likely hit your inode (i.e., file) limits. Please alter the path in the example below to suit your unique needs. Note pytorch-cuda11 in the example below will be the environment name. conda create -p /gscratch/scrubbed/npho/pytorch-cuda11 python=3.8 -y Copy You can see all the conda environments you have regardless of where they are stored. $ conda env list # conda environments: # /gscratch/scrubbed/npho/pytorch-cuda11 base * /usr/lusers/npho/miniconda3 $ Copy To load the environment at this point one would run: conda activate /gscratch/scrubbed/npho/pytorch-cuda11 Copy Note you have to provide the full path. If you use the --name or -n option instead of the --prefix or -p option then by default your environments will be in the same place as your miniconda3 installation, likely your home directory if you used our instructions in the previous section. If you run only 1 environment and use only a few packages this should be fine, otherwise you'll want to use the scrubbed or your lab directories. If you are going to use a non-miniconda3 path regularly then you may want to run a variant of the command below to indicate an additional path that conda should search for your environments. conda config --append envs_dirs /gscratch/scrubbed/npho Copy You need to swap out my scrubbed directory with your lab folder. Now if you list all your conda environments again you should see a change. $ conda env list # conda environments: # pytorch-cuda11 /gscratch/scrubbed/npho/pytorch-cuda11 base * /usr/lusers/npho/miniconda3 $ Copy Adding a new conda environment prefix path will allow your environments to be referred to by their names alone instead of the full path, although you could still use the full path to avoid any ambiguity. Now (for me) loading my conda environment is a more concise command of: conda activate pytorch-cuda11 Copy However, while loading environments will be a shorter command, you will still see the full path in your terminal prompt and this may annoy some folks (it annoys me) so one work around below in lieu of what you just did. caution If you use the alternative method of making your environment references more concise below, then removing environments will be a 2 step process just as making each more concise will be a 2 step process. Instead of adding a path to the envs_dirs variable as demonstrated above you can create a symbolic link for each conda environment after you create it. Modify your link command below as appropriate. ln -s /gscratch/scrubbed/npho/pytorch-cuda10 ~/miniconda3/envs/ Copy In this two step process you will have to create the environment using the --prefix or -p arguments as well as symbolically link it to the env sub-folder where your miniconda3 is installed. $ conda env list # conda environments: # /gscratch/scrubbed/npho/pytorch-cuda11 base * /usr/lusers/npho/miniconda3 pytorch-cuda11 /usr/lusers/npho/miniconda3/envs/pytorch-cuda11 $ Copy The first and third environments above are the same and to remove both will be a two step process, first undoing the symbolic link or conda env remove -n pytorch-cuda11 followed by removing the actual environment by its path reference with conda env remove -p /gscratch/scrubbed/npho/pytorch-cuda11. However, I don't create / remove environments often so this extra step is nice in that I can (un)load my environments concisely and when it is loaded the screen of my terminal prompt isn't consumed. Your level of desire for command line vanity may vary :) "},{"title":"pip install pytorch","type":1,"pageTitle":"Python","url":"docs/tools/python#pip-install-pytorch","content":"If you've followed the full page up til this point you have a blank conda environment using python3.8 and have loaded it using a variant of the following command: conda activate pytorch-cuda11 Copy Pytorch has a great install guide [www] and you can see below it provides what the commands are for whichever platform you are using and which install method you prefer. We're going with pip as it's the most widely known and it demonstrates how easy it is to use in a conda environment. HYAK runs Linux and as of January 2021 CUDA11 is the version on all the GPUs.  To use the install instructions you see above in an easy copy-and-paste format see below from within an activated conda environment: pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html Copy As a reminder these packages are pulled from the internet so you will need a build node or similar to do this part of the install. tip The above pip install commands are equally valid for any other Python libraries. Pytorch doesn't need a GPU to run although for most machine learning projects it's indispensible. If you have an interactive session with a GPU (run nvidia-smi to confirm) you can verify it's all working with the following test from Pytorch [www]: (pytorch-cuda11) $ python3 Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.__version__ '1.7.1+cu110' >>> torch.cuda.is_available() True >>> Copy Now you have a computationally reproducible environment using Pytorch for machine learning. If you want to save this environment and share with others you can export the packages you've installed in it using the conda env export command and provide either the path or name prefixes. The resulting YAML file can be shared and used to create environments elsewhere that conda is installed. "},{"title":"Python Library Repositories","type":1,"pageTitle":"Python","url":"docs/tools/python#python-library-repositories","content":"There are multiple places to search for Python libraries. Whatever libraries you install are specific to whatever conda environment is loaded at the time of install. Anaconda Cloud [www]Python Package Index (PyPI) [www]Conda Forge [www] "},{"title":"Containers","type":1,"pageTitle":"Python","url":"docs/tools/python#containers","content":"TODO "},{"title":"Start Here","type":0,"sectionRef":"#","url":"docs/tools/software","content":"Tool and software are the responsibility of each individual researcher; this is important for you to ensure your own computational reproducibility. That being said, we have some general help to get your started in the following pages.","keywords":""},{"title":"Tutorial","type":0,"sectionRef":"#","url":"docs/tutorial","content":"Probably put something about how to run a simple HYAK job or something here.","keywords":""},{"title":"R and Rstudio","type":0,"sectionRef":"#","url":"docs/tools/r","content":"","keywords":""},{"title":"R","type":1,"pageTitle":"R and Rstudio","url":"docs/tools/r#r","content":"We encourage users to employ the containerized versions of R instead of compiling from source and running bare-metal. We'll use Docker hub containers as that is where the most regular updates to R come from. "},{"title":"User Environment","type":1,"pageTitle":"R and Rstudio","url":"docs/tools/r#user-environment","content":"If you use a non-custom R container you'll likely want to run install.packages() at some point. Usually on a non-shared platform like your local setup (where you have full administrative privileges) R will install things into central paths. You don't want to do that on HYAK so you need to specify user paths. $ cat ~/.Renviron R_LIBS=\"/gscratch/scrubbed/npho/R/\" $ Copy caution If you plan on using multiple R versions you will want to set R_LIBS appropriately with each different container (i.e., R version) used so packages compiled against one version of R don't conflict with another. Using sub-folders with names matching that version of R is sufficient. You can set custom R environment variables with the .Renviron file. I set the R_LIBS environment variable to point to a folder I created in \"scrubbed\" as an example but you will want to use a shared lab space or other path unique to your environment. info If you set R_LIBS to your home directory you can quickly run out of inodes as R likes to create a lot of files. Use your lab directory instead. "},{"title":"Base Container","type":1,"pageTitle":"R and Rstudio","url":"docs/tools/r#base-container","content":"Let's say we wanted to use R-4.0.3 from Docker hub [www]. singularity pull docker://r-base:4.0.3 Copy Be sure to do this from a build node, you need to be routed to the internet to resolve Dockerhub so you can download and have compute resources to do the image conversion from a Docker to Singularity container. $ module load singularity $ singularity pull docker://r-base:4.0.3 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Skipping fetch of repeat blob sha256:ecd924c7226f314c16de965258f37da4aa990c07e494be1116af512706138401 Skipping fetch of repeat blob sha256:4fec22ce03e6be2d27efb3e9a90be68b14183859abf0864518e2581aa49fb8f5 Skipping fetch of repeat blob sha256:69d900227a8f4d4d2647927b9fa8da77f0f535ca497bc771c5f8a72b0cc971df Skipping fetch of repeat blob sha256:45c9ad96a035ae2a5bda28a6e666ed7e1bbe5d945180faafd1c2ed611473e728 Skipping fetch of repeat blob sha256:f6f2416c67056dd5c3a5948858ab4578631e13cc36ee43bd8d44dea4ec4693f7 Skipping fetch of repeat blob sha256:0079bdf52f600362737f2c00e7e7ae11458844ef1d06265caf24115be990c4ab Copying config sha256:729e7d1e2a8b59f2e885f4c7586be463050a70c00cc06bdef306e93a5bf88922 4.11 KiB / 4.11 KiB [======================================================] 0s Writing manifest to image destination Storing signatures 2021/01/10 11:41:34 info unpack layer: sha256:ecd924c7226f314c16de965258f37da4aa990c07e494be1116af512706138401 2021/01/10 11:41:37 info unpack layer: sha256:4fec22ce03e6be2d27efb3e9a90be68b14183859abf0864518e2581aa49fb8f5 2021/01/10 11:41:37 info unpack layer: sha256:69d900227a8f4d4d2647927b9fa8da77f0f535ca497bc771c5f8a72b0cc971df 2021/01/10 11:41:38 info unpack layer: sha256:45c9ad96a035ae2a5bda28a6e666ed7e1bbe5d945180faafd1c2ed611473e728 2021/01/10 11:41:38 info unpack layer: sha256:f6f2416c67056dd5c3a5948858ab4578631e13cc36ee43bd8d44dea4ec4693f7 2021/01/10 11:41:38 info unpack layer: sha256:0079bdf52f600362737f2c00e7e7ae11458844ef1d06265caf24115be990c4ab INFO: Creating SIF file... INFO: Build complete: r-base_4.0.3.sif $ Copy The command will take a minute and create the SIF file in your current directory. $ ls -alh r-base_4.0.3.sif 474M r-base_4.0.3.sif $ Copy You can run the R binary within the container like below. $ singularity run r-base_4.0.3.sif R R version 4.0.3 (2020-10-10) -- \"Bunny-Wunnies Freak Out\" Copyright (C) 2020 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) > library(tidyverse) Error in library(tidyverse) : there is no package called ‘tidyverse’ > Copy You can run install.packages() as you normally would if you were working with R locally and it will install all the files to whatever path you set R_LIBS to in the user environment instructions. "},{"title":"Tidyverse Container","type":1,"pageTitle":"R and Rstudio","url":"docs/tools/r#tidyverse-container","content":"The most popular library for R is the Tidyverse [www], which includes packages like ggplot2, dplyr, and others. As you can see in the previous section, it doesn't exist if we use the r-base Docker hub container. Your options are to: run install.packages(\"tidyverse\") oruse a Docker container with it pre-installed. Option 1, while ok, uses a lot (and I mean a lot) of inodes as well as taking a long time to compile. It's much leaner on the cluster and faster to use a pre-built container if you know you'll use the Tidyverse. The Rocker Project [www] manages popular Docker containers for R, including a pre-built one with Tidyverse so you can grab the latest tagged container from Docker hub [www]. singularity pull docker://rocker/tidyverse:4.0.1 Copy Prior instructions on R user environment apply but once downloaded (the Docker to Singularity conversion will take a few minutes), it will create a separate SIF file as shown below. $ singularity pull docker://rocker/tidyverse:4.0.1 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Skipping fetch of repeat blob sha256:a4a2a29f9ba48efd3d2075f395538b2eec56fb1bedfb7aecf5e54174446f9e2a Skipping fetch of repeat blob sha256:127c9761dcbaa288abc58fc56437c2f2ffbe611b9f7f30e0b5b43cd348bb2094 Skipping fetch of repeat blob sha256:d13bf203e905463e64d89b14509aafa983fb8baf7c1931fe0a65652aeb6c838f Skipping fetch of repeat blob sha256:4039240d2e0b4bcb42ccbce75bc54570e471ad81457478de35fbeef63536e9c0 Copying blob sha256:0ccb9f239bc673ecea30ef11ce2c495bbb85abdd96d675d44f60324c7c45d387 20.56 KiB / 20.56 KiB [====================================================] 0s Copying blob sha256:c01d69634a9de92ab364afa9d6da377ba923131a4e170c3983a9243acafb0879 278.52 MiB / 278.52 MiB [=================================================] 11s Copying blob sha256:d6e983dd45f0e15594336e5aabbcd004fbbb7efd8355ebd5e07f916837659fd6 225.42 MiB / 225.42 MiB [==================================================] 9s Copying blob sha256:0d19f4c62fe952f2cec37dbde49decc2a787df40d87eecd7a8a657c04335c20b 190.84 KiB / 190.84 KiB [==================================================] 0s Copying blob sha256:c4186f33a14ce3205196906179b6d17db22a3e9dfc9f34fedbb1f38bf2e715c9 190.39 MiB / 190.39 MiB [==================================================] 7s Copying config sha256:22b3bdc99b5be46467e695d3639022d344d0fc4daf239a6da1f6b478ed1ee695 6.38 KiB / 6.38 KiB [======================================================] 0s Writing manifest to image destination Storing signatures 2021/01/10 12:28:57 info unpack layer: sha256:a4a2a29f9ba48efd3d2075f395538b2eec56fb1bedfb7aecf5e54174446f9e2a 2021/01/10 12:28:59 info unpack layer: sha256:127c9761dcbaa288abc58fc56437c2f2ffbe611b9f7f30e0b5b43cd348bb2094 2021/01/10 12:28:59 info unpack layer: sha256:d13bf203e905463e64d89b14509aafa983fb8baf7c1931fe0a65652aeb6c838f 2021/01/10 12:28:59 info unpack layer: sha256:4039240d2e0b4bcb42ccbce75bc54570e471ad81457478de35fbeef63536e9c0 2021/01/10 12:28:59 info unpack layer: sha256:0ccb9f239bc673ecea30ef11ce2c495bbb85abdd96d675d44f60324c7c45d387 2021/01/10 12:28:59 info unpack layer: sha256:c01d69634a9de92ab364afa9d6da377ba923131a4e170c3983a9243acafb0879 2021/01/10 12:29:10 info unpack layer: sha256:d6e983dd45f0e15594336e5aabbcd004fbbb7efd8355ebd5e07f916837659fd6 2021/01/10 12:29:22 info unpack layer: sha256:0d19f4c62fe952f2cec37dbde49decc2a787df40d87eecd7a8a657c04335c20b 2021/01/10 12:29:22 info unpack layer: sha256:c4186f33a14ce3205196906179b6d17db22a3e9dfc9f34fedbb1f38bf2e715c9 INFO: Creating SIF file... INFO: Build complete: tidyverse_4.0.1.sif $ ls -alh tidyverse_4.0.1.sif 675M tidyverse_4.0.1.sif $ Copy Now when you run this container's R binary you can successfully load the Tidyverse. $ singularity run tidyverse_4.0.1.sif R R version 4.0.1 (2020-06-06) -- \"See Things Now\" Copyright (C) 2020 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) > library(tidyverse) ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ✔ ggplot2 3.3.2 ✔ purrr 0.3.4 ✔ tibble 3.0.1 ✔ dplyr 1.0.0 ✔ tidyr 1.1.0 ✔ stringr 1.4.0 ✔ readr 1.3.1 ✔ forcats 0.5.0 ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ✖ dplyr::filter() masks stats::filter() ✖ dplyr::lag() masks stats::lag() Warning messages: 1: replacing previous import ‘vctrs::data_frame’ by ‘tibble::data_frame’ when loading ‘dplyr’ 2: package ‘purrr’ was built under R version 4.0.3 > Copy Success! "},{"title":"Module","type":1,"pageTitle":"R and Rstudio","url":"docs/tools/r#module","content":"We've since migrated from bare-metal R binaries compiled from source and provided as a module to leveraging containers. However, there are still some version 3 variants of R still available. $ module avail r_3 contrib/r/ ----- /sw/modules-1.775/modulefiles ----- r_3.3.3 r_3.5.1 r_3.6.0 r_3.6.0+Rmpi-impi_2019 ----- /sw/modules-1.775/modulefiles ----- contrib/r/3.4.3 contrib/r/3.5.1 contrib/r/3.6.1 Copy As a reminder all \"contrib\" prefixed modules are user community created and maintained (i.e., not supported by the HYAK team). "},{"title":"Rstudio","type":1,"pageTitle":"R and Rstudio","url":"docs/tools/r#rstudio","content":"Rstudio is an integrated development environment (IDE) for R. It's a front-end interface, historically a desktop application but it will be delivered through your browser in this instance. Rstudio will run in a Singularity container on a compute node then be directed through the login node back to your local computer via port forwarding. TODO "}]